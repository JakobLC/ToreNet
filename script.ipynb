{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage as nd\n",
    "import tqdm\n",
    "\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_batch(im_size=256, batch_size=32, feature_dim1=76, feature_dim2=32,num_classes=3,\n",
    "                    noisyness_image=0.95,\n",
    "                    noisy_features=0.5):\n",
    "    \n",
    "    ground_truth = np.random.rand(batch_size, num_classes, im_size, im_size)\n",
    "    #blur ground truth and make into class pixel map:\n",
    "    sigma = im_size/10\n",
    "    ground_truth = nd.gaussian_filter(ground_truth, sigma=(0,0,sigma,sigma))\n",
    "    ground_truth[:,0] += ground_truth.std()*1.0\n",
    "    ground_truth = np.argmax(ground_truth, axis=1)[:,None]\n",
    "    image = np.random.rand(batch_size, 1, im_size, im_size)\n",
    "    image = noisyness_image*image + (1-noisyness_image)*ground_truth/num_classes\n",
    "    features = np.random.rand(batch_size, 1, feature_dim1, feature_dim2)\n",
    "    ground_truth_resized = torch.nn.functional.interpolate(torch.tensor(ground_truth).float(), \n",
    "                                                           size=(feature_dim1, feature_dim2), \n",
    "                                                           mode=\"nearest\").numpy()\n",
    "    features = noisy_features*features + (1-noisy_features)*ground_truth_resized/num_classes\n",
    "    return features, image, ground_truth\n",
    "\n",
    "num_dummy_datapoints = 32\n",
    "batch = get_dummy_batch(batch_size=num_dummy_datapoints)\n",
    "\n",
    "#save data in ./data/features, ./data/images, ./data/ground_truth\n",
    "\n",
    "matplotlib_palette = [0,0,0]+sum([[int(round(c2*255)) for c2 in c] for c in plt.get_cmap(\"tab20\").colors][::2],[])\n",
    "folder_names = [\"features\", \"images\", \"ground_truth\"]\n",
    "for i in range(num_dummy_datapoints):\n",
    "    for j in range(len(batch)):\n",
    "        save_path = os.path.join(\"./data\", folder_names[j], f\"{i:06d}.png\")\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        mult = 255 if j < 2 else 1\n",
    "        img = Image.fromarray((batch[j][i][0]*mult).astype(np.uint8))\n",
    "        #put pallete if ground truth\n",
    "        if j == 2:\n",
    "            img = img.convert(\"P\", colors=3)\n",
    "            img.putpalette(matplotlib_palette)\n",
    "        img.save(save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: torch.Size([4, 1, 76, 32])\n",
      "image shape: torch.Size([4, 1, 64, 64])\n",
      "ground truth shape: torch.Size([4, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "#create dataloader for the structure of the data\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir=\"./data\", im_reshape=(64,64)):\n",
    "        self.root_dir = root_dir\n",
    "        self.im_reshape = im_reshape\n",
    "        self.folder_names = [\"features\", \"images\", \"ground_truth\"]\n",
    "        self.filenames = []\n",
    "        for path in list((Path(root_dir)/ self.folder_names[0]).glob(\"*\")):\n",
    "            self.filenames.append(path.name)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        for folder_name in self.folder_names:\n",
    "            path = Path(self.root_dir)/folder_name/filename\n",
    "            if folder_name == self.folder_names[0]:\n",
    "                #load features\n",
    "                features = torch.from_numpy(np.array(Image.open(path))/255).float().unsqueeze(0)\n",
    "            elif folder_name == self.folder_names[1]:\n",
    "                #load image\n",
    "                image = torch.from_numpy(np.array(Image.open(path))/255).float().unsqueeze(0).unsqueeze(0)\n",
    "            elif folder_name == self.folder_names[2]:\n",
    "                #load ground truth\n",
    "                ground_truth = torch.from_numpy(np.array(Image.open(path))).float().unsqueeze(0).unsqueeze(0)\n",
    "        image = torch.nn.functional.interpolate(image, size=self.im_reshape, mode=\"area\").squeeze(0)\n",
    "        ground_truth = torch.nn.functional.interpolate(ground_truth, size=self.im_reshape, mode=\"area\").long().squeeze(0)\n",
    "        sample = [features,image,ground_truth]\n",
    "        return sample\n",
    "\n",
    "dataset = Dataset()\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"features shape:\", batch[0].shape)\n",
    "print(\"image shape:\", batch[1].shape)\n",
    "print(\"ground truth shape:\", batch[2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 1121889 trainable parameters\n",
      "image shape: torch.Size([4, 1, 128, 64])\n",
      "out shape: torch.Size([4, 1, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "import torch\n",
    "\n",
    "unet = UNet2DConditionModel(block_out_channels=[32,32,32,32],encoder_hid_dim=64,cross_attention_dim = 64, in_channels=1, out_channels=1)\n",
    "def number_of_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"model has {num_params} trainable parameters\")\n",
    "\n",
    "number_of_parameters(unet)\n",
    "unet.to(\"cuda\")\n",
    "hidden_states = torch.randn(4, 3, 64).cuda()\n",
    "timesteps = torch.tensor(0).long().cuda()\n",
    "image = torch.randn(4, 1, 128, 64).cuda()\n",
    "print(\"image shape:\", image.shape)\n",
    "out = unet(image, timesteps, hidden_states)\n",
    "print(\"out shape:\", out[\"sample\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 1121889 trainable parameters\n",
      "model has 457216 trainable parameters\n",
      "image shape: torch.Size([4, 1, 128, 64])\n",
      "out shape: torch.Size([4, 32, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "def convert_unet_to_downnet(unet):\n",
    "    unet.up_blocks = torch.nn.ModuleList()\n",
    "    unet.conv_norm_out = None\n",
    "    unet.conv_out = torch.nn.Identity()\n",
    "    return unet\n",
    "\n",
    "unet = UNet2DConditionModel(block_out_channels=[32,32,32,32],encoder_hid_dim=64,cross_attention_dim = 64, in_channels=1, out_channels=1)\n",
    "def number_of_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"model has {num_params} trainable parameters\")\n",
    "\n",
    "number_of_parameters(unet)\n",
    "unet = convert_unet_to_downnet(unet)\n",
    "number_of_parameters(unet)\n",
    "unet.to(\"cuda\")\n",
    "hidden_states = torch.randn(4, 3, 64).cuda()\n",
    "timesteps = torch.tensor(0).long().cuda()\n",
    "image = torch.randn(4, 1, 128, 64).cuda()\n",
    "print(\"image shape:\", image.shape)\n",
    "out = unet(image, timesteps, hidden_states)\n",
    "print(\"out shape:\", out[\"sample\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 5425502 trainable parameters\n",
      "image shape: torch.Size([4, 1, 64, 64])\n",
      "out shape: torch.Size([4, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "def get_default_args():\n",
    "    args = Namespace()\n",
    "    #training args\n",
    "    args.batch_size = 4\n",
    "    args.lr = 0.001\n",
    "    args.num_epochs = 10\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model args\n",
    "    args.im_size=64\n",
    "    args.feature_dim1=76\n",
    "    args.feature_dim2=32\n",
    "    args.block_out_channels = [32,32,64,64,128] #how large the matrices are in the unet\n",
    "    args.num_classes = 3 # number of blobs+ background class\n",
    "\n",
    "    #args you probably dont want to change\n",
    "    args.encoder_hid_dim = 64\n",
    "    args.cross_attention_dim = 64\n",
    "    args.down_block_types = [\"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\"]\n",
    "    args.up_block_types = [\"UpBlock2D\", \"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"]\n",
    "    args.mid_block_type = \"UNetMidBlock2DCrossAttn\"\n",
    "    # One of [\"concat\",\"embed\",\"simple_x_attn\",\"x_attn\"] where each is more complex than the one before. Not sure what is best\n",
    "    args.conditioning_mode = \"x_attn\"\n",
    "    return args\n",
    "\n",
    "def modify_block_names(block_names,conditioning_mode):\n",
    "    if conditioning_mode in [\"concat\",\"embed\"]:\n",
    "        block_names = [x.replace(\"SimpleCrossAttn\",\"\") for x in block_names]\n",
    "        block_names = [x.replace(\"CrossAttn\",\"\") for x in block_names]\n",
    "    elif conditioning_mode == \"simple_x_attn\":\n",
    "        block_names = [x.replace(\"CrossAttn\",\"SimpleCrossAttn\") for x in block_names]\n",
    "    else:\n",
    "        assert conditioning_mode == \"x_attn\"\n",
    "    return block_names\n",
    "\n",
    "def is_power_of_2(num):\n",
    "    power = np.log2(num)\n",
    "    is_power = np.isclose(power,np.round(power))\n",
    "    return np.ceil(power).astype(int),is_power\n",
    "\n",
    "class ToreNet(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        down_block_types = args.down_block_types\n",
    "        while len(args.block_out_channels) != len(down_block_types):\n",
    "            down_block_types = [args.down_block_types[0]] + args.down_block_types\n",
    "        up_block_types = args.up_block_types\n",
    "        while len(args.block_out_channels) != len(up_block_types):\n",
    "            up_block_types = [args.up_block_types[0]] + args.up_block_types\n",
    "        if args.conditioning_mode in [\"concat\",\"embed\"]:\n",
    "            mid_block_type = None\n",
    "        elif args.conditioning_mode == \"simple_x_attn\":\n",
    "            mid_block_type = args.mid_block_type.replace(\"CrossAttn\",\"SimpleCrossAttn\")\n",
    "        elif args.conditioning_mode == \"x_attn\":\n",
    "            mid_block_type = args.mid_block_type\n",
    "        down_block_types = modify_block_names(down_block_types,args.conditioning_mode)\n",
    "        up_block_types = modify_block_names(up_block_types,args.conditioning_mode)\n",
    "        self.image_unet = UNet2DConditionModel(block_out_channels=args.block_out_channels,\n",
    "                                         encoder_hid_dim=args.encoder_hid_dim,\n",
    "                                         cross_attention_dim=args.cross_attention_dim,\n",
    "                                         down_block_types=down_block_types,\n",
    "                                         up_block_types=up_block_types,\n",
    "                                         mid_block_type=mid_block_type,\n",
    "                                         in_channels=2 if args.conditioning_mode == \"concat\" else 1, \n",
    "                                         out_channels=args.num_classes,\n",
    "                                         addition_embed_type=\"text\" if args.conditioning_mode == \"embed\" else None)\n",
    "        assert args.im_size % 2**(len(args.block_out_channels)-1) == 0, \"image size must be divisible by 2**num_down_blocks\"\n",
    "        if args.conditioning_mode in [\"embed\",\"simple_x_attn\",\"x_attn\"]:\n",
    "            down_block_types = modify_block_names(down_block_types,\"embed\")\n",
    "            up_block_types = modify_block_names(up_block_types,\"embed\")\n",
    "            self.feature_downnet = UNet2DConditionModel(block_out_channels=args.block_out_channels[:-1]+[args.encoder_hid_dim],\n",
    "                                         down_block_types=down_block_types,\n",
    "                                         up_block_types=up_block_types,\n",
    "                                         mid_block_type=None,\n",
    "                                         in_channels=2 if args.conditioning_mode == \"concat\" else 1, \n",
    "                                         out_channels=args.num_classes)\n",
    "            self.feature_unet = convert_unet_to_downnet(self.feature_downnet)\n",
    "            \n",
    "            #add so we can accept strange feature sizes\n",
    "            power_ceil,is_power = is_power_of_2(args.feature_dim1)\n",
    "            self.feature_dim1_linear = torch.nn.Identity() if is_power else torch.nn.Linear(args.feature_dim1, power_ceil)\n",
    "            power_ceil,is_power = is_power_of_2(args.feature_dim2)\n",
    "            self.feature_dim2_linear = torch.nn.Identity() if is_power else torch.nn.Linear(args.feature_dim2, power_ceil)\n",
    "\n",
    "    def forward(self, image, features):\n",
    "        dummy_timesteps = torch.tensor(0).long().cuda() #added simply because this library is made for diffusion models and we dont want to rewrite their code\n",
    "        if self.args.conditioning_mode == \"concat\":\n",
    "            features = torch.nn.functional.interpolate(features, size=(image.shape[2],image.shape[3]), mode=\"area\")\n",
    "            x = torch.concat([image, features], dim=1)\n",
    "            x = self.image_unet(x, dummy_timesteps, hidden_states)\n",
    "        elif self.args.conditioning_mode in [\"embed\",\"simple_x_attn\",\"x_attn\"]:\n",
    "            features = self.feature_dim2_linear(features)\n",
    "            features = self.feature_dim1_linear(features.permute(0,1,3,2)).permute(0,1,3,2)\n",
    "            features = self.feature_downnet(features, dummy_timesteps, encoder_hidden_states=None)[\"sample\"]\n",
    "            encoder_hidden_states = torch.nn.functional.avg_pool2d(features, kernel_size=(features.shape[2],features.shape[3]))\n",
    "            encoder_hidden_states = encoder_hidden_states[:,None,:,0,0]\n",
    "            x = self.image_unet(image, dummy_timesteps, encoder_hidden_states=encoder_hidden_states)\n",
    "        else:\n",
    "            raise NotImplementedError(self.args.conditioning_mode)\n",
    "        return x[\"sample\"]\n",
    "\n",
    "args = get_default_args()\n",
    "args.conditioning_mode = \"simple_x_attn\"\n",
    "model = ToreNet(args)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "number_of_parameters(model)\n",
    "features, image, ground_truth = next(iter(train_dataloader))\n",
    "features, image, ground_truth = (features.to(args.device), \n",
    "                                    image.to(args.device), \n",
    "                                    ground_truth.to(args.device))\n",
    "print(\"image shape:\", image.shape)\n",
    "out = model(image, features)\n",
    "print(\"out shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, train_dataloader, val_dataloader, args):\n",
    "    model.train()\n",
    "    pbar = tqdm.tqdm(range(args.num_epochs)*len(train_dataloader))\n",
    "    pbar.set_description(\"training\")\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            features, image, ground_truth = batch\n",
    "            features, image, ground_truth = (features.to(args.device), \n",
    "                                    image.to(args.device), \n",
    "                                    ground_truth.to(args.device))\n",
    "            pred = model(image, features)\n",
    "            loss = torch.nn.functional.cross_entropy(pred, ground_truth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"epoch {epoch}, iter {i}, loss {loss.item()}\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                features, image, ground_truth = batch\n",
    "                features, image, ground_truth = (features.to(args.device), \n",
    "                                                 image.to(args.device), \n",
    "                                                 ground_truth.to(args.device))\n",
    "                pred = model(image, features)\n",
    "                loss = torch.nn.functional.cross_entropy(pred, ground_truth)\n",
    "                val_loss = loss.item()\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"epoch {epoch}, iter {i}, val loss {val_loss}\")\n",
    "            model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "train_loop(model, optimizer, train_dataloader, val_dataloader, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
